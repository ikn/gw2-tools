#! /usr/bin/env bash

source `which env_parallel.bash`
env_parallel --session
shopt -s nullglob

LOGS_PATH="$GW2_DPSREPORT_LOGS_PATH"
URL="${GW2_DPSREPORT_URL:-https://dps.report}"
UPLOAD_PARALLEL_LIMIT="${GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT:-5}"
SIZE_THRESHOLD_CUSTOM="${GW2_DPSREPORT_SIZE_THRESHOLD_CUSTOM:-0}"

# where the day boundary falls, as %H%M%S/%H:%M:%S
DAY_START_NUM=050000
DAY_START_TIME=05:00:00
# if DAY has this value, the program should upload only the most recent log
DAY_VALUE_LATEST_ONLY=latest
# min bytes for standard-format file to be included (2MB/300kB)
SIZE_THRESHOLD_RAW=2000000
SIZE_THRESHOLD_ZIP=300000
TEMP_DIR="$(mktemp -d)"

DAY="$1"

[ -z "$LOGS_PATH" ] && {
    echo >&2 "error: environment variable not set: GW2_DPSREPORT_LOGS_PATH"
    exit 2
}
[ -d "$LOGS_PATH" ] || {
    echo >&2 "error: not a directory: $LOGS_PATH"
    exit 2
}
[[ "$UPLOAD_PARALLEL_LIMIT" =~ ^[0-9]+$ ]] || {
    echo >&2 -n "error: GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT: not a number:"
    echo >&2 "$UPLOAD_PARALLEL_LIMIT"
    exit 2
}
[[ "$SIZE_THRESHOLD_CUSTOM" =~ ^[0-9]+$ ]] || {
    echo >&2 -n "error: GW2_DPSREPORT_SIZE_THRESHOLD_CUSTOM: not a number:"
    echo >&2 "$SIZE_THRESHOLD_CUSTOM"
    exit 2
}
[ "$#" -ne 1 ] && {
    echo >&2 "usage: gw2-dpsreport DAY"
    echo >&2 "DAY: eg. 'today', 'last wednesday', '2001-02-03'"
    exit 2
}
[ "$DAY" = "$DAY_VALUE_LATEST_ONLY" ] ||
    date &> /dev/null --date="$DAY" "+%Y%m%d %H:%M:%S" ||
    {
        echo >&2 "error: invalid day: $DAY"
        exit 2
    }
type -p 7z > /dev/null || {
    echo >&2 -n "warning: 7z not found; "
    echo >&2 "logs compressed in formats other than Zip will be ignored; "
    echo >&2 "uncompressed logs will be uploaded without compression"
}

# given log file path, print boss name
parse_path_boss_name () {
    local f="$1"
    local dir_name="$(dirname "$f")"
    echo "${dir_name##*/}"
}

# remove all created temporary files
cleanup () {
    rm -rf "$TEMP_DIR"
}

# filter log files to those that were recorded on the day specified in $DAY
# read lines: FILE_PATH
# write lines: FILE_PATH
filter_logs_time () {
    local num_format="%Y%m%d%H%M%S"
    local time_num="$(date --date="$DAY" +%H%M%S)"
    local date_unixtime="$(date --date="$DAY" +%s)"
    local day="$(date --date="$DAY" +%Y%m%d)"

    # move back 1 day if time falls before the threshold
    # however, if time is midnight exactly, it's very likely that the user
    # specified a date without a time, in which case we don't want to move back
    local start_num
    local end_num
    if [ "$time_num" -lt "$DAY_START_NUM" ] && [ "$time_num" != 000000 ]; then
        local day_start="$(
            date --date="@$((date_unixtime - 24*60*60))" "+%Y%m%d")"
        start_num="$(date --date="$day_start $DAY_START_TIME" +"$num_format")"
        end_num="$(date --date="$day $DAY_START_TIME" +"$num_format")"
    else
        local day_end="$(
            date --date="@$((date_unixtime + 24*60*60))" "+%Y%m%d")"
        start_num="$(date --date="$day $DAY_START_TIME" +"$num_format")"
        end_num="$(date --date="$day_end $DAY_START_TIME" +"$num_format")"
    fi

    local f
    while read -r f; do
        local name="${f##*/}"
        local file_date_raw="${name%%.*}" # %Y%m%d-%H%M%S
        local file_num="${file_date_raw%-*}${file_date_raw#*-}"
        [ "$file_num" -ge "$start_num" ] &&
            [ "$file_num" -lt "$end_num" ] &&
            echo "$f"
    done
}

# filter log files to those above the small-file threshold
# read lines: FILE_PATH
# write lines: FILE_PATH
filter_logs_size () {
    local f
    while read -r f; do
        local size="$(stat --format=%s "$f")"
        local ext="${f##*.}"
        local limit
        if [ "$ext" = evtc ]; then
            limit="$SIZE_THRESHOLD_RAW"
        elif [ "$ext" = zip ]; then
            limit="$SIZE_THRESHOLD_ZIP"
        else
            limit="$SIZE_THRESHOLD_CUSTOM"
        fi

        if [ "$size" -ge "$limit" ]; then
            echo "$f"
        fi
    done
}

# filter log files to the last attempt at each encounter, count attempts, and
# sort by date
# read lines: FILE_PATH
# write lines: NUM_BOSS_ATTEMPTS FILE_PATH
filter_logs_grouped () {
    sort | {
        local prev_boss_name=
        local num_attempts=0
        local prev_f
        local f

        while read -r f; do
            local boss_name="$(parse_path_boss_name "$f")"
            if [ "$boss_name" = "$prev_boss_name" ]; then
                num_attempts="$((num_attempts + 1))"
            else
                [ "$num_attempts" != 0 ] &&
                    echo "$num_attempts $prev_f"
                prev_boss_name="$boss_name"
                num_attempts=1
            fi
            prev_f="$f"
        done

        [ "$num_attempts" != 0 ] &&
            echo "$num_attempts $prev_f"

    }
}

# sort logs in ascending order by date
# read and write lines: NUM_BOSS_ATTEMPTS FILE_PATH
sort_logs_date () {
    local num_attempts
    local f
    while read -r num_attempts f; do
        local name="${f##*/}"
        local date="${name%.*}"
        echo "${date/-/} $num_attempts $f"
    done |
        sort -n |
        while read -r date num_attempts f; do
            echo "$num_attempts" "$f"
        done
}

# decompress_log LOG_FILE
#
# Decompress LOG_FILE, if necessary, and write the result to a temporary file.
# Prints the new file path, which might be LOG_FILE (also on failure).
decompress_log () {
    local file="$1"
    local ext="${file##*.}"
    local tmp_file

    if type -p 7z > /dev/null &&
        [ "$ext" != evtc ] && [ "$ext" != zip ] &&
        # preserve the correct extension to send it to the server, which uses it
        # to determine file type
        tmp_file="$(mktemp -p "$TEMP_DIR" -q --suffix=.evtc)" &&
        7z e -y -bsp0 -bso0 -so "$file" > "$tmp_file"
    then
        echo "$tmp_file"
    else
        [ -n "$tmp_file" ] && rm -f "$tmp_file"
        echo "$file"
    fi
}

# compress_log LOG_FILE
#
# Compress LOG_FILE, if necessary, and write the result to a temporary file.
# Prints the new file path, which might be LOG_FILE (also on failure), and has
# an appropriate file extension.
compress_log () {
    local file="$1"
    local tmp_file

    if type -p 7z > /dev/null &&
        [ "${file##*.}" = evtc ] &&
        # preserve the correct extension to send it to the server, which uses it
        # to determine file type
        tmp_file="$(mktemp -p "$TEMP_DIR" -u -q --suffix=.evtc.zip)" &&
        7z a -y -bsp0 -bso0 -tzip "$tmp_file" "$file"
    then
        echo "$tmp_file"
    else
        [ -n "$tmp_file" ] && rm -f "$tmp_file"
        echo "$file"
    fi
}

# upload log file at the given path
#
# write line: RESULT URL
# RESULT: adjective describing whether the encounter attempt succeeded
# This also written also on error, in which case RESULT is unknown and URL is
# the error message.
#
# Writes to file descriptor 3 with standard error output from `curl`, with
# carriage returns replaced with newlines to allow for line buffering of
# progress updates
upload_log () {
    local orig_file="$1"
    local tmp_file1="$(decompress_log "$orig_file")"
    local tmp_file2="$(compress_log "$tmp_file1")"
    local file="$tmp_file2"
    # preserve the original filename in the upload - the server puts it in the
    # report URL
    local orig_filename="$(basename "$orig_file")"
    local upload_name="${orig_filename%.*}.${file##*.}"

    local url="$URL"/uploadContent
    local res
    res="$(curl 2> >(stdbuf -oL tr '\r' '\n' >&3) "$url" \
        -F file="@$file;filename=$upload_name" \
        -F json=1)"
    local err_code="$?"
    [ "$tmp_file1" != "$orig_file" ] && rm -f "$tmp_file1"
    [ "$tmp_file2" != "$orig_file" ] && rm -f "$tmp_file2"
    [ "$err_code" -ne 0 ] && {
        echo unknown "$res"
        return 1
    }

    local err
    err="$(jshon -Q -e error <<<"$res")"
    if [ "$?" != 0 ] || [ "$err" != null ]; then
        echo unknown "${err:-$res}"
        return 1
    fi

    local result=unknown
    local success
    if success="$(jshon -Q -e encounter -e success <<<"$res")"; then
        if [ "$success" = true ]; then
            result=success
        elif [ "$success" = false ]; then
            result=failure
        fi
    fi

    local link
    link="$(jshon -Q -e permalink -u <<<"$res")" || {
        echo unknown "unexpected HTTP response: $res"
        return 1
    }

    echo "$result $link"
}

# combine progress updates from multiple `curl` invocations, given the total
# number of invocations
#
# read lines: ID UPDATE
#
# ID: the number of the `curl` invocation that this progress update comes from
# UPDATE: the line (ie. split by carriage returns and new lines) written to
#         standard error by `curl`
#
# Writes the total progress percentage to standard output whenever an update is
# received, intended for writing to a terminal.
combine_upload_progress () {
    local num_items="$1"
    {
        local total_progress=0 # last known sum of progress percentages
        local items_progress=() # last known progress percentages by item ID
        local item
        local progress
        local line # unused, just capture the rest of the line
        local wrote_progress=n # track whether we ever write the progress

        while read -r item progress line; do
            [[ "$progress" =~ ^[0-9]+$ ]] || continue
            local previous_item_progress="${items_progress[$item]:-0}"
            total_progress="$((
                total_progress + progress - previous_item_progress))"
            items_progress[$item]="$progress"
            echo -ne "\r$((total_progress / num_items))%"
            wrote_progress=y
        done

        # move the cursor to the next line if we wrote anything
        [ "$wrote_progress" = y ] && echo
    }
}

# upload log files; failed uploads (RESULT = unknown) are printed with an error
# message instead of the URL
# read lines: NUM_BOSS_ATTEMPTS FILE_PATH
# write lines: NUM_BOSS_ATTEMPTS/BOSS_NAME/RESULT/(URL|ERROR)
upload_logs () {
    # need to read in all lines before we start, to count them for progress
    # reporting
    local input="$(cat)"
    local num_logs="$(wc -l <<<"$input")"
    if [ -n "$input" ]; then echo "$input"; fi |
        # upload in parallel
        # preserving the order by prepending the sequence number
        # preserving the input line's attempts count by prepending
        # generating boss name since we don't preserve the file path
        # combining progress bars for terminal output
        # replacing only the first space from `upload_log` with / since boss
        # name can contain spaces
        env_parallel --line-buffer --jobs "$UPLOAD_PARALLEL_LIMIT" -- \
            'echo -n {#}/{= $_ =~ s/ .+// =}/$(parse_path_boss_name {= $_ =~ s/^.+? // =})/;' \
            'upload_log {= $_ =~ s/^.+? // =}' \
            '    3> >(stdbuf -oL sed "s/^/{#} /" >&3)' \
            '    | sed "s: :/:"' \
            3> >(combine_upload_progress "$num_logs" >&2) |
            sort -n |
        {
            local num
            local num_attempts
            local boss_name
            local result
            local url
            while IFS=/ read -r num num_attempts boss_name result url; do
                echo "$num_attempts/$boss_name/$result/$url"
            done
        }
}

# read standard input until EOF, then print it
buffer_input () {
    local input="$(cat)"
    if [ -n "$input" ]; then
        echo >&2
        echo "$input"
    else
        echo >&2 "(no logs found)"
    fi
}

# print log upload results in a human-readable format
# read lines: NUM_BOSS_ATTEMPTS/BOSS_NAME/RESULT/(URL|ERROR)
display_results () {
    local num_attempts
    local boss_name
    local result
    local url

    while IFS=/ read -r num_attempts boss_name result url; do
        if [ "$DAY" = "$DAY_VALUE_LATEST_ONLY" ]; then
            echo -n "$boss_name ($result): "
        else
            echo -n "$boss_name (attempts: $num_attempts, $result): "
        fi

        if [ "$result" = unknown ]; then
            echo "[upload failed]"
            echo >&2 "upload failure reason: $url"
        else
            echo "$url"
        fi
    done
}

trap cleanup TERM INT EXIT
for f in "$LOGS_PATH"/*/*.evtc{,.*}; do echo "$f"; done |
    if [ "$DAY" = "$DAY_VALUE_LATEST_ONLY" ]; then
        # dummy attempt count value - ignored in `display_results`
        while read -r f; do echo 0 "$f"; done |
            sort_logs_date |
            tail -n1
    else
        filter_logs_time |
            filter_logs_size |
            filter_logs_grouped |
            sort_logs_date
    fi |
    upload_logs |
    buffer_input |
    display_results
for c in "${PIPESTATUS[@]}"; do
    [ "$c" != 0 ] && exit "$c"
done
