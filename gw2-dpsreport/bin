#! /usr/bin/env bash

source `which env_parallel.bash`
env_parallel --session
shopt -s nullglob

LOGS_PATH="$GW2_DPSREPORT_LOGS_PATH"
URL="${GW2_DPSREPORT_URL:-https://dps.report}"
UPLOAD_PARALLEL_LIMIT="${GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT:-5}"

# where the day boundary falls, as %H%M%S/%H:%M:%S
DAY_START_NUM=050000
DAY_START_TIME=05:00:00
# if DAY has this value, the program should upload only the most recent log
DAY_VALUE_LATEST_ONLY=latest
# min bytes for file to be counted as an attempt (2MB/300kB)
ATTEMPTS_THRESHOLD_RAW=2000000
ATTEMPTS_THRESHOLD_ZIP=300000
TEMP_DIR="$(mktemp -d)"

DAY="$1"

[ -z "$LOGS_PATH" ] && {
    echo >&2 "error: environment variable not set: GW2_DPSREPORT_LOGS_PATH"
    exit 2
}
[ -d "$LOGS_PATH" ] || {
    echo >&2 "error: not a directory: $LOGS_PATH"
    exit 2
}
[[ "$UPLOAD_PARALLEL_LIMIT" =~ ^[0-9]+$ ]] || {
    echo >&2 -n "error: GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT: not a number:"
    echo >&2 "$UPLOAD_PARALLEL_LIMIT"
    exit 2
}
[ "$#" -ne 1 ] && {
    echo >&2 "usage: gw2-dpsreport DAY"
    echo >&2 "DAY: eg. 'today', 'last wednesday', '2001-02-03'"
    exit 2
}
[ "$DAY" = "$DAY_VALUE_LATEST_ONLY" ] ||
    date &> /dev/null --date="$DAY" "+%Y%m%d %H:%M:%S" ||
    {
        echo >&2 "error: invalid day: $DAY"
        exit 2
    }
type -p zip > /dev/null || {
    echo >&2 -n "warning: zip not found; "
    echo >&2 "uncompressed logs will be uploaded without compression"
}

# given log file path, print boss name
parse_path_boss_name () {
    local f="$1"
    echo "$(basename "$(dirname "$f")")"
}

# given log file path, print date as %Y%m%d%H%M%S
parse_path_date () {
    local f="$1"
    local date="$(basename "$f")"
    date="${date/-/}"
    echo "${date%.*}"
}

# remove all created temporary files
cleanup () {
    rm -rf "$TEMP_DIR"
}

# filter log files to those that were recorded on the day specified in $DAY
# read lines: FILE_PATH
# write lines: FILE_PATH
filter_logs_time () {
    local time_num="$(date --date="$DAY" +%H%M%S)"
    local date_unixtime="$(date --date="$DAY" +%s)"
    local day="$(date --date="$DAY" +%Y%m%d)"

    # move back 1 day if time falls before the threshold
    # however, if time is midnight exactly, it's very likely that the user
    # specified a date without a time, in which case we don't want to move back
    local start_unixtime
    local end_unixtime
    if [ "$time_num" -lt "$DAY_START_NUM" ] && [ "$time_num" != 000000 ]; then
        local day_start="$(
            date --date="@$((date_unixtime - 24*60*60))" "+%Y%m%d")"
        start_unixtime="$(date --date="$day_start $DAY_START_TIME" +%s)"
        end_unixtime="$(date --date="$day $DAY_START_TIME" +%s)"
    else
        local day_end="$(
            date --date="@$((date_unixtime + 24*60*60))" "+%Y%m%d")"
        start_unixtime="$(date --date="$day $DAY_START_TIME" +%s)"
        end_unixtime="$(date --date="$day_end $DAY_START_TIME" +%s)"
    fi

    local f
    while read -r f; do
        local name="$(basename "$f")"
        local file_date_raw="${name%%.*}" # %Y%m%d-%H%M%S
        local file_time="${file_date_raw#*-}" # %H%M%S
        local file_date="${file_date_raw%-*} ${file_time:0:2}:${file_time:2:2}:${file_time:4:2}"
        local file_unixtime="$(date --date="$file_date" +%s)"
        [ "$file_unixtime" -ge "$start_unixtime" ] &&
            [ "$file_unixtime" -lt "$end_unixtime" ] &&
            echo "$f"
    done
}

# filter log files to those above the small-file threshold
# read lines: FILE_PATH
# write lines: FILE_PATH
filter_logs_size () {
    local f
    while read -r f; do
        local size="$(stat --format=%s "$f")"
        local ext="${f##*.}"
        local limit
        if [ "$ext" = evtc ]; then
            limit="$ATTEMPTS_THRESHOLD_RAW"
        elif [ "$ext" = zip ]; then
            limit="$ATTEMPTS_THRESHOLD_ZIP"
        else
            limit=0
        fi

        if [ "$size" -ge "$limit" ]; then
            echo "$f"
        fi
    done
}

# filter log files to the last attempt at each encounter, count attempts, and
# sort by date
# read lines: FILE_PATH
# write lines: NUM_BOSS_ATTEMPTS FILE_PATH
filter_logs_grouped () {
    {
        local prev_boss_name=
        local num_attempts=0
        local prev_f
        local f

        while read -r f; do
            local boss_name="$(parse_path_boss_name "$f")"
            if [ "$boss_name" = "$prev_boss_name" ]; then
                num_attempts="$((num_attempts + 1))"
            else
                [ "$num_attempts" != 0 ] &&
                    echo "$(parse_path_date "$prev_f") $num_attempts $prev_f"
                prev_boss_name="$boss_name"
                num_attempts=1
            fi
            prev_f="$f"
        done

        [ "$num_attempts" != 0 ] &&
            echo "$(parse_path_date "$prev_f") $num_attempts $prev_f"

    } < <(sort) |
        sort -n |
        while read -r date num_attempts f; do
            echo "$num_attempts" "$f"
        done
}

# compress_log LOG_FILE TMP_FILE
#
# compress LOG_FILE, if necessary, and write the result to TMP_FILE
# prints the new file path, which might be LOG_FILE or TMP_FILE
compress_log () {
    local file="$1"
    local tmp_file="$2"
    local ext="${file##*.}"

    if type -p zip > /dev/null &&
        [ "$ext" = evtc ] &&
        zip -q - -- "$file" > "$tmp_file"
    then
        echo "$tmp_file"
    else
        echo "$file"
    fi
}

# upload log file at the given path
#
# write line: RESULT URL
# RESULT: adjective describing whether the encounter attempt succeeded
# This also written also on error, in which case RESULT is unknown and URL is
# the error message.
#
# Standard error output comes from `curl`, with carriage returns replaced with
# newlines to allow for line buffering of progress updates
upload_log () {
    local orig_file="$1"
    local tmp_file
    local file
    # preserve the correct extension to send it to the server, which uses it to
    # determine file type
    if tmp_file="$(mktemp -p "$TEMP_DIR" -q --suffix=.zip)"; then
        file="$(compress_log "$orig_file" "$tmp_file")"
    else
        file="$orig_file"
    fi
    # preserve the original filename in the upload - the server puts it in the
    # report URL
    local orig_filename="$(basename "$orig_file")"
    local upload_name="${orig_filename%.*}.${file##*.}"

    local url="$URL"/uploadContent
    local res
    res="$(curl 2> >(stdbuf -oL tr '\r' '\n' >&2) "$url" \
        -F file="@$file;filename=$upload_name" \
        -F json=1)"
    local err_code="$?"
    [ -n "$tmp_file" ] && rm -f "$tmp_file"
    [ "$err_code" -ne 0 ] && return 1

    local err
    err="$(jshon -Q -e error <<<"$res")"
    if [ "$?" != 0 ] || [ "$err" != null ]; then
        echo unknown "${err:-$res}"
        return 1
    fi

    local result=unknown
    local success
    if success="$(jshon -Q -e encounter -e success <<<"$res")"; then
        if [ "$success" = true ]; then
            result=success
        elif [ "$success" = false ]; then
            result=failure
        fi
    fi

    local link
    link="$(jshon -Q -e permalink -u <<<"$res")" || {
        echo unknown "unexpected HTTP response: $res"
        return 1
    }

    echo "$result $link"
}

# combine progress updates from multiple `curl` invocations, given the number
# of 
#
# read lines: ID UPDATE
#
# ID: the number of the `curl` invocation that this progress update comes from
# UPDATE: the line (ie. split by carriage returns and new lines) written to
#         standard error by `curl`
#
# Writes the total progress percentage to standard output whenever an update is
# received, intended for writing to a terminal.
combine_upload_progress () {
    local num_items="$1"
    {
        local total_progress=0 # last known sum of progress percentages
        local items_progress=() # last known progress percentages by item ID
        local item
        local progress
        local line # unused, just capture the rest of the line
        local wrote_progress=n # track whether we ever write the progress
        while read -r item progress line; do
            [[ "$progress" =~ ^[0-9]+$ ]] || continue
            local previous_item_progress="${items_progress[$item]:-0}"
            total_progress="$((total_progress + progress - previous_item_progress))"
            items_progress[$item]="$progress"
            echo >&2 -ne "\r$((total_progress / num_items))%"
            wrote_progress=y
        done

        # move the cursor to the next line if we wrote anything
        [ "$wrote_progress" = y ] && echo >&2
    }
}

# upload log files; failed uploads (RESULT = unknown) are printed with an error
# message instead of the URL
# read lines: NUM_BOSS_ATTEMPTS FILE_PATH
# write lines: NUM_BOSS_ATTEMPTS/BOSS_NAME/RESULT/(URL|ERROR)
upload_logs () {
    # need to read in all lines before we start, to count them for progress
    # reporting
    local input="$(cat)"
    local num_logs="$(wc -l <<<"$input")"
    if [ -n "$input" ]; then echo "$input"; fi |
        # upload in parallel
        # preserving the input line's attempts count by prepending
        # generating boss name since we don't preserve the file path
        # combining progress bars for terminal output
        # replacing only the first space from `upload_log` with / since boss
        # name can contain spaces
        env_parallel --line-buffer --jobs "$UPLOAD_PARALLEL_LIMIT" -- \
            'echo -n {= $_ =~ s/ .+// =}/$(parse_path_boss_name {= $_ =~ s/^.+? // =})/;' \
            'upload_log {= $_ =~ s/^.+? // =}' \
            '    2> >(stdbuf -oL sed "s/^/{#} /" >&2)' \
            '    | sed "s: :/:"' \
            2> >(combine_upload_progress "$num_logs" >&2) |
        {
            local num_attempts
            local boss_name
            local result
            local url
            while IFS=/ read -r num_attempts boss_name result url; do
                echo "$num_attempts/$boss_name/$result/$url"
            done
        }
}

# read standard input until EOF, then print it
buffer_input () {
    local input="$(cat)"
    if [ -n "$input" ]; then
        echo >&2
        echo "$input"
    else
        echo >&2 "(no logs found)"
    fi
}

# print log upload results in a human-readable format
# read lines: NUM_BOSS_ATTEMPTS/BOSS_NAME/RESULT/(URL|ERROR)
display_results () {
    local num_attempts
    local boss_name
    local result
    local url
    while IFS=/ read -r num_attempts boss_name result url; do
        echo -n "$boss_name (attempts: $num_attempts, $result): "
        if [ "$result" = unknown ]; then
            echo "[upload failed]"
            echo >&2 "upload failure reason: $url"
        else
            echo "$url"
        fi
    done
}

trap cleanup TERM INT EXIT
for f in "$LOGS_PATH"/*/*.evtc{,.zip}; do echo "$f"; done |
    if [ "$DAY" = "$DAY_VALUE_LATEST_ONLY" ]; then
        cat
    else
        filter_logs_time
    fi |
    filter_logs_size |
    filter_logs_grouped |
    if [ "$DAY" = "$DAY_VALUE_LATEST_ONLY" ]; then
        tail -n1
    else
        cat
    fi |
    upload_logs |
    buffer_input |
    display_results
for c in "${PIPESTATUS[@]}"; do
    [ "$c" != 0 ] && exit "$c"
done
