#! /usr/bin/env bash

source `which env_parallel.bash`
env_parallel --session
shopt -s nullglob

PROGRAM="gw2-dpsreport"
VERSION="1.8-next"
SHORT_OPTIONS="hcp:f:b"
LONG_OPTIONS="version,help,clipboard,display-format:,on-failure:,open-in-browser"
ERR_OTHER=1
ERR_USAGE=2
# where the day boundary falls, as %H%M%S/%H:%M:%S
DAY_START_NUM=050000
DAY_START_TIME=05:00:00
# if DAY has this value, the program should upload only the most recent log
DAY_VALUE_LATEST_ONLY=latest
TEMP_DIR="$(mktemp -d)"

env_logs_path=
env_url="https://dps.report"
env_upload_parallel_limit=5
env_upload_max_attempts=5
env_clipboard=primary
env_browser=
env_token=

arg_day=

opt_clipboard=n
opt_display_format=brief
opt_on_failure=last
opt_open_in_browser=n


err () {
    code="$1"
    shift
    echo >&2 "$PROGRAM: error:" "$@"
    exit "$code"
}


usage_err () {
    if [ "$#" -gt 0 ]; then
        echo >&2 "$PROGRAM: error:" "$@"
    fi
    echo >&2
    echo >&2 "Call with \`--help' to display usage information."
    exit "$ERR_USAGE"
}


print_version () {
    echo "$PROGRAM $VERSION"
}


print_help() {
    print_version
    # end-of-line marker ->                                                               |
    echo
    echo "Upload a day's arcdps logs to \`dps.report'."
    echo
    echo "Usage: $PROGRAM [OPTION]... DAY"
    echo
    echo "DAY is the day to upload logs for, as accepted by \`date', (e.g. 'today', 'last"
    echo "wednesday', 'friday 3 weeks ago', '2001-02-03').  You can also use the special"
    echo "value 'latest', which means only upload the single most recent log, from any"
    echo "day."
    echo
    echo "- Uploads the last log file for each encounter in the specified day, regardless"
    echo "  of whether or not there was a successful attempt."
    echo "- A 'day' starts and ends at 5am, in your local timezone."
    echo "- Logs may be uncompressed, or compressed using any format supported by p7zip"
    echo "  (if it's installed).  File extension is used to determine whether to"
    echo "  decompress/compress."
    echo
    echo "Options:"
    echo "  -c, --clipboard"
    echo "                copy the result to clipboard"
    echo "  -p, --display-format=FORMAT"
    echo "                how to display the uploaded logs (see 'Display formats' below)"
    echo "  -f, --on-failure=ACTION"
    echo "                what to do if the last log for an encounter isn't a"
    echo "                successful attempt (see 'Failure actions' below)"
    echo "  -b, --open-in-browser"
    echo "                open reports in the configured web browser (see"
    echo "                GW2_DPSREPORT_WEB_BROWSER)"
    echo "  -h, --help    display this help and exit"
    echo "  --version     display version information and exit"
    echo
    echo "Environment variables:"
    echo "  GW2_DPSREPORT_LOGS_PATH or WINEPREFIX (required)"
    echo "      directory containing the encounter directories, or try to work out this"
    echo "      directory from the Wine data directory"
    echo "  GW2_DPSREPORT_URL (default: https://dps.report)"
    echo "      the base URL to upload logs to"
    echo "  GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT (default: 5)"
    echo "      maximum allowed parallel uploads (0 means no maximum)"
    echo "  GW2_DPSREPORT_UPLOAD_MAX_ATTEMPTS (default: 5)"
    echo "      retry if an upload fails, up to this many total attempts"
    echo "  GW2_DPSREPORT_CLIPBOARD (default: primary)"
    echo "      when specifying \`--clipboard', use this X selection (see xclip's"
    echo "      \`-selection' option)"
    echo "  GW2_DPSREPORT_WEB_BROWSER (required with --open-in-browser)"
    echo "      when specifying \`--open-in-browser', pass URLs as separate arguments to"
    echo "      this command; may include extra arguments, and arguments may not contain"
    echo "      whitespace"
    echo "  GW2_DPSREPORT_USER_TOKEN"
    echo "      associate uploaded logs with a specific user"
    echo
    echo "Display formats (default: readable):"
    echo "  brief         for each encounter, display encounter name, sucess/failure,"
    echo "                number of attempts, and URL"
    echo "  url-only      for each encounter, display only the URL"
    echo
    echo "Failure actions (default: last):"
    echo "  last          upload the most recent log for the encounter"
    echo "  skip          don't upload any logs for the encounter"
}


read_env () {
    env_logs_path="$GW2_DPSREPORT_LOGS_PATH"
    [ -z "$env_logs_path" ] && {
        if [ -n "$WINEPREFIX" ]; then
            wine_docs="$WINEPREFIX/drive_c/users/$USER/My Documents"
            env_logs_path="$wine_docs/Guild Wars 2/addons/arcdps/arcdps.cbtlogs"
        else
            usage_err "environment variable not set: GW2_DPSREPORT_LOGS_PATH"
        fi
    }
    [ -d "$env_logs_path" ] || {
        usage_err "not a directory: $env_logs_path"
    }

    env_url="${GW2_DPSREPORT_URL:-$env_url}"

    env_upload_parallel_limit="${GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT:-$env_upload_parallel_limit}"
    [[ "$env_upload_parallel_limit" =~ ^[0-9]+$ ]] || {
        usage_err "GW2_DPSREPORT_UPLOAD_PARALLEL_LIMIT: not a number:" \
            "$env_upload_parallel_limit"
    }

    env_upload_max_attempts="${GW2_DPSREPORT_UPLOAD_MAX_ATTEMPTS:-$env_upload_max_attempts}"
    [[ "$env_upload_max_attempts" =~ ^[0-9]+$ ]] || {
        usage_err "GW2_DPSREPORT_UPLOAD_MAX_ATTEMPTS: not a number:" \
            "$env_upload_max_attempts"
    }

    env_clipboard="${GW2_DPSREPORT_CLIPBOARD:-$env_clipboard}"

    env_browser="$GW2_DPSREPORT_WEB_BROWSER"

    env_token="$GW2_DPSREPORT_USER_TOKEN"

    type -p 7z > /dev/null || {
        echo >&2 -n "warning: 7z not found; "
        echo >&2 -n "logs compressed in formats other than Zip will be ignored; "
        echo >&2 "uncompressed logs will be uploaded without compression"
    }
}


read_args () {
    [ "$#" -gt 1 ] && usage_err "too many arguments: expected 1"
    [ "$#" -lt 1 ] && usage_err "missing argument: DAY"

    arg_day="$1"
    [ "$arg_day" = "$DAY_VALUE_LATEST_ONLY" ] ||
        date &> /dev/null --date="$arg_day" "+%Y%m%d %H:%M:%S" ||
        {
            usage_err "invalid value for DAY: $arg_day"
        }
}


read_options () {
    while true; do
        case "$1" in
            '--version')
                print_version
                exit 0
                ;;
            '-h'|'--help')
                print_help
                exit 0
                ;;
            '-c'|'--clipboard')
                opt_clipboard=y
                shift
                ;;
            '-p'|'--display-format')
                opt_display_format="$2"
                shift 2
                ;;
            '-f'|'--on-failure')
                opt_on_failure="$2"
                shift 2
                ;;
            '-b'|'--open-in-browser')
                opt_open_in_browser=y
                shift
                ;;
            '--')
                shift
                break
                ;;
        esac
    done

    read_env # after args so we can exit early with --help/--version

    if [ "$opt_clipboard" = y ] && ! type -p xclip > /dev/null; then
        err "$ERR_OTHER" \
            "\`--clipboard' option provided but xclip is not available"
    fi

    if [ "$opt_display_format" != brief ] &&
        [ "$opt_display_format" != url-only ]
    then
        usage_err "\`--display-format': invalid argument: $opt_display_format"
    fi

    if [ "$opt_on_failure" != last ] && [ "$opt_on_failure" != skip ]; then
        usage_err "\`--on-failure': invalid argument: $opt_on_failure"
    fi

    if [ "$opt_open_in_browser" = y ] && [ -z "$env_browser" ]; then
        usage_err "\`--open-in-browser' option provided but" \
            "GW2_DPSREPORT_WEB_BROWSER is not set"
    fi

    read_args "$@"
}


# given log file path, print boss name
parse_path_boss_name () {
    local f="$1"
    local dir_name="$(dirname "$f")"
    echo "${dir_name##*/}"
}


# remove all created temporary files
cleanup () {
    rm -rf "$TEMP_DIR"
}


# filter log files to those that were recorded on the day specified in $arg_day
# read lines: FILE_PATH
# write lines: FILE_PATH
filter_logs_time () {
    local num_format="%Y%m%d%H%M%S"
    local time_num="$(date --date="$arg_day" +%H%M%S)"
    local date_unixtime="$(date --date="$arg_day" +%s)"
    local day="$(date --date="$arg_day" +%Y%m%d)"

    # move back 1 day if time falls before the threshold
    # however, if time is midnight exactly, it's very likely that the user
    # specified a date without a time, in which case we don't want to move back
    local start_num
    local end_num
    if [ "$time_num" -lt "$DAY_START_NUM" ] && [ "$time_num" != 000000 ]; then
        local day_start="$(
            date --date="@$((date_unixtime - 24*60*60))" "+%Y%m%d")"
        start_num="$(date --date="$day_start $DAY_START_TIME" +"$num_format")"
        end_num="$(date --date="$day $DAY_START_TIME" +"$num_format")"
    else
        local day_end="$(
            date --date="@$((date_unixtime + 24*60*60))" "+%Y%m%d")"
        start_num="$(date --date="$day $DAY_START_TIME" +"$num_format")"
        end_num="$(date --date="$day_end $DAY_START_TIME" +"$num_format")"
    fi

    local f
    while read -r f; do
        local name="${f##*/}"
        local file_date_raw="${name%%.*}" # %Y%m%d-%H%M%S
        local file_num="${file_date_raw%-*}${file_date_raw#*-}"
        [ "$file_num" -ge "$start_num" ] &&
            [ "$file_num" -lt "$end_num" ] &&
            echo "$f"
    done
}


# filter log files to the last attempt at each encounter, count attempts, and
# sort by date
# read lines: FILE_PATH
# write lines: NUM_BOSS_ATTEMPTS FILE_PATH
filter_logs_grouped () {
    sort | {
        local prev_boss_name=
        local num_attempts=0
        local prev_f
        local f

        while read -r f; do
            local boss_name="$(parse_path_boss_name "$f")"
            if [ "$boss_name" = "$prev_boss_name" ]; then
                num_attempts="$((num_attempts + 1))"
            else
                [ "$num_attempts" != 0 ] &&
                    echo "$num_attempts $prev_f"
                prev_boss_name="$boss_name"
                num_attempts=1
            fi
            prev_f="$f"
        done

        [ "$num_attempts" != 0 ] &&
            echo "$num_attempts $prev_f"

    }
}


# sort logs in ascending order by date
# read and write lines: NUM_BOSS_ATTEMPTS FILE_PATH
sort_logs_date () {
    local num_attempts
    local f
    while read -r num_attempts f; do
        local name="${f##*/}"
        local date="${name%.*}"
        echo "${date/-/} $num_attempts $f"
    done |
        sort -n |
        while read -r date num_attempts f; do
            echo "$num_attempts" "$f"
        done
}


# decompress_log LOG_FILE
#
# Decompress LOG_FILE, if necessary, and write the result to a temporary file.
# Prints the new file path, which might be LOG_FILE (also on failure).
decompress_log () {
    local file="$1"
    local ext="${file##*.}"
    local tmp_file

    if type -p 7z > /dev/null &&
        [ "$ext" != evtc ] && [ "$ext" != zip ] &&
        # preserve the correct extension to send it to the server, which uses it
        # to determine file type
        tmp_file="$(mktemp -p "$TEMP_DIR" -q --suffix=.evtc)" &&
        7z e -y -bsp0 -bso0 -so "$file" > "$tmp_file"
    then
        echo "$tmp_file"
    else
        [ -n "$tmp_file" ] && rm -f "$tmp_file"
        echo "$file"
    fi
}


# compress_log LOG_FILE
#
# Compress LOG_FILE, if necessary, and write the result to a temporary file.
# Prints the new file path, which might be LOG_FILE (also on failure), and has
# an appropriate file extension.
compress_log () {
    local file="$1"
    local tmp_file

    if type -p 7z > /dev/null &&
        [ "${file##*.}" = evtc ] &&
        # preserve the correct extension to send it to the server, which uses it
        # to determine file type
        tmp_file="$(mktemp -p "$TEMP_DIR" -u -q --suffix=.evtc.zip)" &&
        7z a -y -bsp0 -bso0 -tzip "$tmp_file" "$file"
    then
        echo "$tmp_file"
    else
        [ -n "$tmp_file" ] && rm -f "$tmp_file"
        echo "$file"
    fi
}


# upload log file at the given path
#
# write line: RESULT/BOSS_NAME/URL
# RESULT: adjective describing whether the encounter attempt succeeded
# This is also written on error, in which case RESULT is unknown and URL is
# the error message.
#
# Writes to file descriptor 3 with standard error output from `curl`, with
# carriage returns replaced with newlines to allow for line buffering of
# progress updates
upload_log () {
    local newline=$'\n'
    local orig_file="$1"
    local file_boss_name="$(parse_path_boss_name "$orig_file")"
    local tmp_file1="$(decompress_log "$orig_file")"
    local tmp_file2="$(compress_log "$tmp_file1")"
    local file="$tmp_file2"
    # preserve the original filename in the upload - the server puts it in the
    # report URL
    local orig_filename="$(basename "$orig_file")"
    local upload_name="${orig_filename%.*}.${file##*.}"

    local url="$env_url"/uploadContent
    local res
    res="$(curl 2> >(stdbuf -oL tr '\r' '\n' >&3) "$url" \
        -F file="@$file;filename=$upload_name" \
        -F userToken="$env_token" \
        -F json=1)"
    local err_code="$?"
    [ "$tmp_file1" != "$orig_file" ] && rm -f "$tmp_file1"
    [ "$tmp_file2" != "$orig_file" ] && rm -f "$tmp_file2"
    [ "$err_code" -ne 0 ] && {
        echo "unknown/$file_boss_name/${res//$newline/|}"
        return "$ERR_OTHER"
    }

    local err
    err="$(jshon -Q -e error <<<"$res")"
    if [ "$?" != 0 ] || [ "$err" != null ]; then
        err="${err//$newline/|}"
        echo "unknown/$file_boss_name/${err:-${res//$newline/|}}"
        return "$ERR_OTHER"
    fi

    local result=unknown
    local success
    if success="$(jshon -Q -e encounter -e success <<<"$res")"; then
        if [ "$success" = true ]; then
            result=success
        elif [ "$success" = false ]; then
            result=failure
        fi
    fi

    local boss_name
    boss_name="$(jshon -Q -e encounter -e boss -u <<<"$res" | sed 's:/: :')"
    [ -z "$boss_name" ] && boss_name="$file_boss_name"

    local is_cm
    is_cm="$(jshon -Q -e encounter -e isCm -u <<<"$res")"
    [ "$is_cm" = true ] && boss_name="$boss_name CM"

    local link
    link="$(jshon -Q -e permalink -u <<<"$res")" || {
        echo "unknown/$boss_name/unexpected HTTP response: ${res//$newline/|}"
        return "$ERR_OTHER"
    }

    echo "$result/$boss_name/$link"
}


# like `upload_log`, with retries
upload_log_retry () {
    local attempts=0
    local res
    local err_code
    while true; do
        attempts="$((attempts + 1))"
        res="$(upload_log "$@")"
        err_code="$?"
        if [ "$err_code" -eq 0 ] ||
           [ "$attempts" -ge "$env_upload_max_attempts" ]
        then
            echo "$res"
            return "$err_code"
        fi
    done
}


# combine progress updates from multiple `curl` invocations, given the total
# number of invocations
#
# read lines: ID UPDATE
#
# ID: the number of the `curl` invocation that this progress update comes from
# UPDATE: the line (ie. split by carriage returns and new lines) written to
#         standard error by `curl`
#
# Writes the total progress percentage to standard output whenever an update is
# received, intended for writing to a terminal.
combine_upload_progress () {
    local num_items="$1"
    {
        local total_progress=0 # last known sum of progress percentages
        local items_progress=() # last known progress percentages by item ID
        local item
        local progress
        local line # unused, just capture the rest of the line
        local wrote_progress=n # track whether we ever write the progress

        while read -r item progress line; do
            [[ "$progress" =~ ^[0-9]+$ ]] || continue
            local previous_item_progress="${items_progress[$item]:-0}"
            total_progress="$((
                total_progress + progress - previous_item_progress))"
            items_progress[$item]="$progress"
            echo -ne "\r$((total_progress / num_items))%"
            wrote_progress=y
        done

        # move the cursor to the next line if we wrote anything
        [ "$wrote_progress" = y ] && echo
    }
}


# upload log files; failed uploads (RESULT = unknown) are printed with an error
# message instead of the URL
# read lines: NUM_BOSS_ATTEMPTS FILE_PATH
# write lines: NUM_BOSS_ATTEMPTS/RESULT/BOSS_NAME/(URL|ERROR)
upload_logs () {
    # need to read in all lines before we start, to count them for progress
    # reporting
    local input="$(cat)"
    local num_logs="$(wc -l <<<"$input")"
    if [ -n "$input" ]; then echo "$input"; fi |
        # upload in parallel
        # preserving the order by prepending the sequence number
        # preserving the input line's attempts count by prepending
        # generating boss name since we don't preserve the file path
        # combining progress bars for terminal output
        # replacing only the first space from `upload_log` with / since boss
        # name can contain spaces
        env_parallel --line-buffer --jobs "$env_upload_parallel_limit" -- \
            'echo -n {#}/{= $_ =~ s/ .+// =}/;' \
            'upload_log_retry {= $_ =~ s/^.+? // =}' \
            '    3> >(stdbuf -oL sed "s/^/{#} /" >&3)' \
            3> >(combine_upload_progress "$num_logs" >&2) |
            sort -n |
        {
            local num
            local num_attempts
            local result
            local boss_name
            local url
            while IFS=/ read -r num num_attempts result boss_name url; do
                if [ "$opt_on_failure" != skip ] ||
                    [ "$result" != failure ]
                then
                    echo "$num_attempts/$result/$boss_name/$url"
                fi
            done
        }
}


# read standard input until EOF, then print it
buffer_input () {
    local input="$(cat)"
    if [ -n "$input" ]; then
        echo >&2
        echo "$input"
    else
        echo >&2 "(no logs found)"
    fi
}


# open reports in the configured web browser, if requested
# read and write lines: NUM_BOSS_ATTEMPTS/RESULT/BOSS_NAME/URL|ERROR)
open_results () {
    local urls=()
    while IFS=/ read -r num_attempts result boss_name url; do
        urls+=("$url")
        echo "$num_attempts/$result/$boss_name/$url"
    done

    if [ "${#urls[@]}" -gt 0 ] && [ "$opt_open_in_browser" = y ]; then
        # it would be nice to be able to rely on xdg-open, but it only supports
        # 1 argument, and in our use case we will almost always have multiple
        # URLs, and we probably care about ordering
        # we must consume stdout or the next function along will hang reading it
        $GW2_DPSREPORT_WEB_BROWSER "${urls[@]}" >&2 &
        disown
    fi
}


# print log upload results in a human-readable format
# read lines: NUM_BOSS_ATTEMPTS/RESULT/BOSS_NAME/URL|ERROR)
display_results () {
    local num_attempts
    local result
    local boss_name
    local url

    while IFS=/ read -r num_attempts result boss_name url; do
        if [ "$opt_display_format" = brief ]; then
            if [ "$arg_day" = "$DAY_VALUE_LATEST_ONLY" ]; then
                echo -n "$boss_name ($result): "
            else
                echo -n "$boss_name (attempts: $num_attempts, $result): "
            fi
        fi

        if [ "$result" = unknown ]; then
            echo "[upload failed]"
            echo >&2 "upload failure reason: $url"
        else
            echo "$url"
        fi
    done
}


# read result text from standard input, copy it to the clipboard if requested,
# and print it again
copy_results () {
    if [ "$opt_clipboard" = y ]; then
        xclip -filter -rmlastnl -selection "$env_clipboard"
        echo
    else
        cat
    fi
}


dpsreport () {
    trap cleanup TERM INT EXIT
    find -L "$env_logs_path" -type f \
        -name '*.zevtc' -o -name '*.evtc' -o -name '*.evtc.*' |
        if [ "$arg_day" = "$DAY_VALUE_LATEST_ONLY" ]; then
            # dummy attempt count value - ignored in `display_results`
            while read -r f; do echo 0 "$f"; done |
                sort_logs_date |
                tail -n1
        else
            filter_logs_time |
                filter_logs_grouped |
                sort_logs_date
        fi |
        upload_logs |
        buffer_input |
        open_results |
        display_results |
        copy_results
    for c in "${PIPESTATUS[@]}"; do
        [ "$c" != 0 ] && return "$c"
    done
}


getopt -T &> /dev/null
if [ "$?" -ne 4 ]; then
    err "$ERR_OTHER" "unsupported version of \`getopt'"
fi
options_script="$(getopt --name "$PROGRAM" --shell bash \
    --options "$SHORT_OPTIONS" --longoptions "$LONG_OPTIONS" -- "$@")"
getopt_code="$?"
if [ "$getopt_code" -eq 1 ]; then
    usage_err
elif [ "$getopt_code" -ne 0 ]; then
    exit 1
fi
eval set -- "$options_script"
read_options "$@"
read_env # after options so it can exit early with --help/--version

dpsreport
exit "$?"
